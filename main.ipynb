{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "# os.environ['HF_TOKEN'] = \"your huggingface token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\n",
    "model = model.eval()\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = 'path/to/test/image.png'\n",
    "res = model.chat(tokenizer, image_file, ocr_type='ocr')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GotEncoder(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vit = model.model.vision_tower_high\n",
    "        self.mm_proj = model.model.mm_projector_vary\n",
    "    def forward(self,x):\n",
    "        return self.mm_proj(self.vit(x))\n",
    "encoder = GotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GotEncoder(\n",
       "  (vit): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "    (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (mm_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type GOT to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151860, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151860, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.qwen2 import Qwen2Config,Qwen2Model,Qwen2ForCausalLM,Qwen2Tokenizer\n",
    "decoder = Qwen2ForCausalLM.from_pretrained('ucaslcl/GOT-OCR2_0',device_map=\"cuda\")\n",
    "decoder.eval()\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_input = torch.ones([1,256],dtype=torch.int64,device=\"cuda\")\n",
    "decoder_output =  decoder.model(input_ids = fake_input).last_hidden_state\n",
    "model_output = model.model(input_ids = fake_input).last_hidden_state\n",
    "decoder_output,model_output\n",
    "torch.allclose(decoder_output,model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./got_decoder_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"qwen.tiktoken\"}\n",
    "\n",
    "PAT_STR = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "IMSTART = \"<|im_start|>\"\n",
    "IMEND = \"<|im_end|>\"\n",
    "# as the default behavior is changed to allow special tokens in\n",
    "# regular texts, the surface forms of special tokens need to be\n",
    "# as different as possible to minimize the impact\n",
    "EXTRAS = tuple((f\"<|extra_{i}|>\" for i in range(205)))\n",
    "SPECIAL_TOKENS = (\n",
    "    ENDOFTEXT,\n",
    "    IMSTART,\n",
    "    IMEND,\n",
    ") + EXTRAS\n",
    "\n",
    "\n",
    "\n",
    "def _load_tiktoken_bpe(tiktoken_bpe_file: str):\n",
    "    with open(tiktoken_bpe_file, \"rb\") as f:\n",
    "        contents = f.read()\n",
    "    return {\n",
    "        base64.b64decode(token): int(rank)\n",
    "        for token, rank in (line.split() for line in contents.splitlines() if line)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"./got_decoder_tmp/qwen.tiktoken\"\n",
    "errors=\"replace\"\n",
    "image_start_tag='<img>'\n",
    "image_end_tag='</img>'\n",
    "image_pad_tag='<imgpad>'\n",
    "ref_start_tag='<ref>'\n",
    "ref_end_tag='</ref>'\n",
    "box_start_tag='<box>'\n",
    "box_end_tag='</box>'\n",
    "quad_start_tag='<quad>'\n",
    "quad_end_tag='</quad>'\n",
    "IMAGE_ST = (\n",
    "            ref_start_tag, ref_end_tag,\n",
    "            box_start_tag, box_end_tag,\n",
    "            quad_start_tag, quad_end_tag,\n",
    "            image_start_tag, image_end_tag,\n",
    "            image_pad_tag\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'Qwen'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tiktoken\n",
    "from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n",
    "mergeable_ranks = _load_tiktoken_bpe(vocab_file)\n",
    "special_tokens = {\n",
    "            token: index\n",
    "            for index, token in enumerate(\n",
    "                SPECIAL_TOKENS + IMAGE_ST, start=len(mergeable_ranks)\n",
    "            )\n",
    "        }\n",
    "\n",
    "enc = tiktoken.Encoding(\n",
    "    \"Qwen\",\n",
    "    pat_str=PAT_STR,\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "decoder = {\n",
    "    v: k for k, v in mergeable_ranks.items()\n",
    "}  # type: dict[int, bytes|str]\n",
    "decoder.update({v: k for k, v in special_tokens.items()})\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tiktoken_to_fast(enc, \"./got_decoder_tokenizer_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.models\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"./got_tokenizer\", trust_remote_code=True)\n",
    "decoder_tokenizer.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./got_decoder/tokenizer_config.json',\n",
       " './got_decoder/special_tokens_map.json',\n",
       " './got_decoder/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_path = \"./got_decoder\"\n",
    "decoder.save_pretrained(decoder_path)\n",
    "decoder_tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n",
    "# from tiktoken import get_encoding\n",
    "# get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "byte_encoder = bytes_to_unicode()\n",
    "def token_bytes_to_string(b):\n",
    "  return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n",
    "\n",
    "# Adapted from https://github.com/openai/tiktoken/issues/60#issuecomment-1499977960\n",
    "def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank = None) -> list[bytes]:\n",
    "  parts = [bytes([b]) for b in token]\n",
    "  while True:\n",
    "    min_idx = None\n",
    "    min_rank = None\n",
    "    for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "      rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "      if rank is not None and (min_rank is None or rank < min_rank):\n",
    "        min_idx = i\n",
    "        min_rank = rank\n",
    "    if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "      break\n",
    "    assert min_idx is not None\n",
    "    parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "  return parts\n",
    "def generate_vocab_and_merges(encoder):\n",
    "  mergeable_ranks = encoder._mergeable_ranks\n",
    "\n",
    "  merges = []\n",
    "  vocab = {}\n",
    "  for token, rank in mergeable_ranks.items():\n",
    "    vocab[token_bytes_to_string(token)] = rank\n",
    "\n",
    "    if len(token) == 1:\n",
    "      continue\n",
    "    merged = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "    assert len(merged) == 2\n",
    "\n",
    "    merges.append(' '.join(map(token_bytes_to_string, merged)))\n",
    "\n",
    "  # Also add special tokens\n",
    "  vocab.update(encoder._special_tokens)\n",
    "\n",
    "  return vocab, merges\n",
    "added_tokens = [\n",
    "    {\n",
    "      \"id\": id,\n",
    "      \"content\": content,\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True,\n",
    "    }\n",
    "    for content, id in enc._special_tokens.items()\n",
    "  ]\n",
    "\n",
    "pre_tokenizer = {\n",
    "    \"type\": \"Sequence\",\n",
    "    \"pretokenizers\": [\n",
    "      {\n",
    "        \"type\": \"Split\",\n",
    "        \"pattern\": {\n",
    "          \"Regex\": \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
    "        },\n",
    "        \"behavior\": \"Removed\",\n",
    "        \"invert\": True\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"ByteLevel\",\n",
    "        \"add_prefix_space\": False,\n",
    "        \"trim_offsets\": True,\n",
    "        \"use_regex\": False\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "decoder_cfg = {\n",
    "    \"type\": \"ByteLevel\",\n",
    "    \"add_prefix_space\": True,\n",
    "    \"trim_offsets\": True,\n",
    "    \"use_regex\": True\n",
    "}\n",
    "\n",
    "with open(\"got_decoder_tokenizer_fast/tokenizer.json\",\"r+\") as f:\n",
    "    import json\n",
    "    d = json.load(f)\n",
    "    d[\"model\"]['vocab'],d[\"model\"]['merges'] = generate_vocab_and_merges(enc)\n",
    "    d['added_tokens'] = added_tokens\n",
    "    \n",
    "    d[\"pre_tokenizer\"]=pre_tokenizer\n",
    "    d[\"decoder\"] = decoder_cfg\n",
    "with open(\"got_decoder_tokenizer_fast/tokenizer.json\",\"w\") as f:\n",
    "    import json\n",
    "    json.dump(d,f,ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "t = PreTrainedTokenizerFast.from_pretrained(\"./got_decoder_tokenizer_fast\")\n",
    "# o = \n",
    "# t.save_pretrained(\"got_decoder_tokenizer_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "added_tokens = [\n",
    "    {\n",
    "      \"id\": id,\n",
    "      \"content\": content,\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True,\n",
    "    }\n",
    "    for content, id in enc._special_tokens.items()\n",
    "  ]\n",
    "added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Requirement already satisfied: filelock>=3.0 in /home//miniforge3/envs/got/lib/python3.10/site-packages (from blobfile) (3.16.1)\"\n",
    "a = t(text)\n",
    "b= tokenizer(text)\n",
    "# c = enc.encode(text)\n",
    "a['input_ids']==b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('got_tokenizer/tokenizer_config.json',\n",
       " 'got_tokenizer/special_tokens_map.json',\n",
       " 'got_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.save_pretrained(\"got_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Type\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ImageEncoderViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 1024,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        out_chans: int = 256,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_abs_pos: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        global_attn_indexes: Tuple[int, ...] = (),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Input image size.\n",
    "            patch_size (int): Patch size.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "            depth (int): Depth of ViT.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks.\n",
    "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embed: Optional[nn.Parameter] = None\n",
    "        if use_abs_pos:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            block = Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                use_rel_pos=use_rel_pos,\n",
    "                rel_pos_zero_init=rel_pos_zero_init,\n",
    "                window_size=window_size if i not in global_attn_indexes else 0,\n",
    "                input_size=(img_size // patch_size, img_size // patch_size),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dim,\n",
    "                out_chans,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "            nn.Conv2d(\n",
    "                out_chans,\n",
    "                out_chans,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.net_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.net_3 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.neck(x.permute(0, 3, 1, 2))\n",
    "        x = self.net_2(x)\n",
    "        x = self.net_3(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
    "                use global attention.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            rel_pos_zero_init=rel_pos_zero_init,\n",
    "            input_size=input_size if window_size == 0 else (window_size, window_size),\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.attn(x)\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n",
    "\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n",
    "        (Hp, Wp): padded height and width before partition\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Hp, Wp)\n",
    "\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Window unpartition into original sequences and removing padding.\n",
    "    Args:\n",
    "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n",
    "        window_size (int): window size.\n",
    "        pad_hw (Tuple): padded height and width (Hp, Wp).\n",
    "        hw (Tuple): original height and width (H, W) before padding.\n",
    "\n",
    "    Returns:\n",
    "        x: unpartitioned sequences with [B, H, W, C].\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "\n",
    "    if Hp > H or Wp > W:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]\n",
    "\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    attn: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attn (Tensor): attention map.\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    attn = (\n",
    "        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return attn\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: Tuple[int, int] = (16, 16),\n",
    "        stride: Tuple[int, int] = (16, 16),\n",
    "        padding: Tuple[int, int] = (0, 0),\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel_size (Tuple): kernel size of the projection layer.\n",
    "            stride (Tuple): stride of the projection layer.\n",
    "            padding (Tuple): padding size of the projection layer.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        # B C H W -> B H W C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def build_GOT_vit_b(checkpoint=None):\n",
    "    return _build_GOT_vision(\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        encoder_num_heads=12,\n",
    "        encoder_global_attn_indexes=[2, 5, 8, 11],\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_GOT_vision(\n",
    "    encoder_embed_dim,\n",
    "    encoder_depth,\n",
    "    encoder_num_heads,\n",
    "    encoder_global_attn_indexes,\n",
    "    checkpoint=None,\n",
    "):\n",
    "    prompt_embed_dim = 256\n",
    "    image_size = 1024\n",
    "    vit_patch_size = 16\n",
    "    image_embedding_size = image_size // vit_patch_size\n",
    "    image_encoder=ImageEncoderViT(\n",
    "            depth=encoder_depth,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            img_size=image_size,\n",
    "            mlp_ratio=4,\n",
    "            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "            num_heads=encoder_num_heads,\n",
    "            patch_size=vit_patch_size,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            global_attn_indexes=encoder_global_attn_indexes,\n",
    "            window_size=14,\n",
    "            out_chans=prompt_embed_dim,\n",
    "        )\n",
    "    \n",
    "\n",
    "    return image_encoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "got",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
