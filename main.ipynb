{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./got_tokenizer', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "        You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
      "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
      "OCR: <|im_end|><|im_start|>assistant\n",
      "\n",
      "image.shape torch.Size([1, 3, 1024, 1024])\n",
      "input: [ 1.9306641  1.9306641  1.9306641 ...  0.4267578 -1.7919922 -1.7919922]\n",
      "encoder: [ 0.96489114  0.6991708  -0.41135037 ... -0.2880915   0.16502841\n",
      " -0.86253846]\n",
      "torch.Size([1, 256, 1024])\n",
      "torch.Size([287]) torch.Size([287, 1024]) torch.Size([1, 256, 1024])\n",
      "torch.Size([]) torch.Size([256, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sd f hd ish'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_file = 'test.png'\n",
    "image_file = 'PixPin_2025-01-08_00-22-53.png'\n",
    "image_file = 'test.jpeg'\n",
    "res = model.chat(tokenizer, image_file, ocr_type='ocr')\n",
    "res\n",
    "# tokenizer.convert_ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GotEncoder(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vit = model.model.vision_tower_high\n",
    "        self.mm_proj = model.model.mm_projector_vary\n",
    "    def forward(self,x):\n",
    "        return self.mm_proj(self.vit(x))\n",
    "encoder = GotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GotEncoder(\n",
       "  (vit): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "    (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (mm_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type GOT to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151860, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151860, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.qwen2 import Qwen2Config,Qwen2Model,Qwen2ForCausalLM,Qwen2Tokenizer\n",
    "decoder = Qwen2ForCausalLM.from_pretrained('ucaslcl/GOT-OCR2_0',device_map=\"cuda\")\n",
    "decoder.eval()\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_input = torch.ones([1,256],dtype=torch.int64,device=\"cuda\")\n",
    "decoder_output =  decoder.model(input_ids = fake_input).last_hidden_state\n",
    "model_output = model.model(input_ids = fake_input).last_hidden_state\n",
    "decoder_output,model_output\n",
    "torch.allclose(decoder_output,model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers.models\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"./got_tokenizer\", trust_remote_code=True)\n",
    "decoder_tokenizer.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./got_decoder/tokenizer_config.json',\n",
       " './got_decoder/special_tokens_map.json',\n",
       " './got_decoder/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_path = \"./got_decoder\"\n",
    "decoder.save_pretrained(decoder_path)\n",
    "decoder_tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n",
    "from tiktoken import get_encoding\n",
    "get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"qwen.tiktoken\"}\n",
    "\n",
    "PAT_STR = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "IMSTART = \"<|im_start|>\"\n",
    "IMEND = \"<|im_end|>\"\n",
    "# as the default behavior is changed to allow special tokens in\n",
    "# regular texts, the surface forms of special tokens need to be\n",
    "# as different as possible to minimize the impact\n",
    "EXTRAS = tuple((f\"<|extra_{i}|>\" for i in range(205)))\n",
    "SPECIAL_TOKENS = (\n",
    "    ENDOFTEXT,\n",
    "    IMSTART,\n",
    "    IMEND,\n",
    ") + EXTRAS\n",
    "\n",
    "\n",
    "import base64\n",
    "def _load_tiktoken_bpe(tiktoken_bpe_file: str):\n",
    "    with open(tiktoken_bpe_file, \"rb\") as f:\n",
    "        contents = f.read()\n",
    "    return {\n",
    "        base64.b64decode(token): int(rank)\n",
    "        for token, rank in (line.split() for line in contents.splitlines() if line)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"./got_decoder/qwen.tiktoken\"\n",
    "errors=\"replace\"\n",
    "image_start_tag='<img>'\n",
    "image_end_tag='</img>'\n",
    "image_pad_tag='<imgpad>'\n",
    "ref_start_tag='<ref>'\n",
    "ref_end_tag='</ref>'\n",
    "box_start_tag='<box>'\n",
    "box_end_tag='</box>'\n",
    "quad_start_tag='<quad>'\n",
    "quad_end_tag='</quad>'\n",
    "IMAGE_ST = (\n",
    "            ref_start_tag, ref_end_tag,\n",
    "            box_start_tag, box_end_tag,\n",
    "            quad_start_tag, quad_end_tag,\n",
    "            image_start_tag, image_end_tag,\n",
    "            image_pad_tag\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'Qwen'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mergeable_ranks = _load_tiktoken_bpe(vocab_file)\n",
    "special_tokens = {\n",
    "            token: index\n",
    "            for index, token in enumerate(\n",
    "                SPECIAL_TOKENS + IMAGE_ST, start=len(mergeable_ranks)\n",
    "            )\n",
    "        }\n",
    "import tiktoken\n",
    "enc = tiktoken.Encoding(\n",
    "    \"Qwen\",\n",
    "    pat_str=PAT_STR,\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "decoder = {\n",
    "    v: k for k, v in mergeable_ranks.items()\n",
    "}  # type: dict[int, bytes|str]\n",
    "decoder.update({v: k for k, v in special_tokens.items()})\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tiktoken_to_fast(enc, \"./tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "t = PreTrainedTokenizerFast.from_pretrained(\"./got_decoder_tokenizer_fast\")\n",
    "# o = \n",
    "# t.save_pretrained(\"got_decoder_tokenizer_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('got_tokenizer/tokenizer_config.json',\n",
       " 'got_tokenizer/special_tokens_map.json',\n",
       " 'got_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.save_pretrained(\"got_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Requirement already satisfied: filelock>=3.0 in /home/wenhongli/miniforge3/envs/got/lib/python3.10/site-packages (from blobfile) (3.16.1)\"\n",
    "a = t(text)\n",
    "b= tokenizer(text)\n",
    "# c = enc.encode(text)\n",
    "a['input_ids']==b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "byte_encoder = bytes_to_unicode()\n",
    "def token_bytes_to_string(b):\n",
    "  return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])\n",
    "\n",
    "# Adapted from https://github.com/openai/tiktoken/issues/60#issuecomment-1499977960\n",
    "def bpe(mergeable_ranks: dict[bytes, int], token: bytes, max_rank = None) -> list[bytes]:\n",
    "  parts = [bytes([b]) for b in token]\n",
    "  while True:\n",
    "    min_idx = None\n",
    "    min_rank = None\n",
    "    for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "      rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "      if rank is not None and (min_rank is None or rank < min_rank):\n",
    "        min_idx = i\n",
    "        min_rank = rank\n",
    "    if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "      break\n",
    "    assert min_idx is not None\n",
    "    parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "  return parts\n",
    "def generate_vocab_and_merges(encoder):\n",
    "  mergeable_ranks = encoder._mergeable_ranks\n",
    "\n",
    "  merges = []\n",
    "  vocab = {}\n",
    "  for token, rank in mergeable_ranks.items():\n",
    "    vocab[token_bytes_to_string(token)] = rank\n",
    "\n",
    "    if len(token) == 1:\n",
    "      continue\n",
    "    merged = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "    assert len(merged) == 2\n",
    "\n",
    "    merges.append(' '.join(map(token_bytes_to_string, merged)))\n",
    "\n",
    "  # Also add special tokens\n",
    "  vocab.update(encoder._special_tokens)\n",
    "\n",
    "  return vocab, merges\n",
    "added_tokens = [\n",
    "    {\n",
    "      \"id\": id,\n",
    "      \"content\": content,\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True,\n",
    "    }\n",
    "    for content, id in enc._special_tokens.items()\n",
    "  ]\n",
    "with open(\"got_decoder_tokenizer_fast/tokenizer.json\",\"r+\") as f:\n",
    "    import json\n",
    "    d = json.load(f)\n",
    "    d[\"model\"]['vocab'],d[\"model\"]['merges'] = generate_vocab_and_merges(enc)\n",
    "    d['added_tokens'] = added_tokens\n",
    "with open(\"got_decoder_tokenizer_fast/tokenizer.json\",\"w\") as f:\n",
    "    import json\n",
    "    json.dump(d,f,ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 151643,\n",
       "  'content': '<|endoftext|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151644,\n",
       "  'content': '<|im_start|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151645,\n",
       "  'content': '<|im_end|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151646,\n",
       "  'content': '<|extra_0|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151647,\n",
       "  'content': '<|extra_1|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151648,\n",
       "  'content': '<|extra_2|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151649,\n",
       "  'content': '<|extra_3|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151650,\n",
       "  'content': '<|extra_4|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151651,\n",
       "  'content': '<|extra_5|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151652,\n",
       "  'content': '<|extra_6|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151653,\n",
       "  'content': '<|extra_7|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151654,\n",
       "  'content': '<|extra_8|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151655,\n",
       "  'content': '<|extra_9|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151656,\n",
       "  'content': '<|extra_10|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151657,\n",
       "  'content': '<|extra_11|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151658,\n",
       "  'content': '<|extra_12|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151659,\n",
       "  'content': '<|extra_13|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151660,\n",
       "  'content': '<|extra_14|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151661,\n",
       "  'content': '<|extra_15|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151662,\n",
       "  'content': '<|extra_16|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151663,\n",
       "  'content': '<|extra_17|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151664,\n",
       "  'content': '<|extra_18|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151665,\n",
       "  'content': '<|extra_19|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151666,\n",
       "  'content': '<|extra_20|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151667,\n",
       "  'content': '<|extra_21|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151668,\n",
       "  'content': '<|extra_22|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151669,\n",
       "  'content': '<|extra_23|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151670,\n",
       "  'content': '<|extra_24|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151671,\n",
       "  'content': '<|extra_25|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151672,\n",
       "  'content': '<|extra_26|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151673,\n",
       "  'content': '<|extra_27|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151674,\n",
       "  'content': '<|extra_28|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151675,\n",
       "  'content': '<|extra_29|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151676,\n",
       "  'content': '<|extra_30|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151677,\n",
       "  'content': '<|extra_31|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151678,\n",
       "  'content': '<|extra_32|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151679,\n",
       "  'content': '<|extra_33|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151680,\n",
       "  'content': '<|extra_34|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151681,\n",
       "  'content': '<|extra_35|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151682,\n",
       "  'content': '<|extra_36|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151683,\n",
       "  'content': '<|extra_37|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151684,\n",
       "  'content': '<|extra_38|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151685,\n",
       "  'content': '<|extra_39|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151686,\n",
       "  'content': '<|extra_40|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151687,\n",
       "  'content': '<|extra_41|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151688,\n",
       "  'content': '<|extra_42|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151689,\n",
       "  'content': '<|extra_43|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151690,\n",
       "  'content': '<|extra_44|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151691,\n",
       "  'content': '<|extra_45|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151692,\n",
       "  'content': '<|extra_46|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151693,\n",
       "  'content': '<|extra_47|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151694,\n",
       "  'content': '<|extra_48|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151695,\n",
       "  'content': '<|extra_49|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151696,\n",
       "  'content': '<|extra_50|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151697,\n",
       "  'content': '<|extra_51|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151698,\n",
       "  'content': '<|extra_52|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151699,\n",
       "  'content': '<|extra_53|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151700,\n",
       "  'content': '<|extra_54|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151701,\n",
       "  'content': '<|extra_55|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151702,\n",
       "  'content': '<|extra_56|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151703,\n",
       "  'content': '<|extra_57|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151704,\n",
       "  'content': '<|extra_58|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151705,\n",
       "  'content': '<|extra_59|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151706,\n",
       "  'content': '<|extra_60|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151707,\n",
       "  'content': '<|extra_61|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151708,\n",
       "  'content': '<|extra_62|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151709,\n",
       "  'content': '<|extra_63|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151710,\n",
       "  'content': '<|extra_64|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151711,\n",
       "  'content': '<|extra_65|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151712,\n",
       "  'content': '<|extra_66|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151713,\n",
       "  'content': '<|extra_67|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151714,\n",
       "  'content': '<|extra_68|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151715,\n",
       "  'content': '<|extra_69|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151716,\n",
       "  'content': '<|extra_70|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151717,\n",
       "  'content': '<|extra_71|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151718,\n",
       "  'content': '<|extra_72|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151719,\n",
       "  'content': '<|extra_73|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151720,\n",
       "  'content': '<|extra_74|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151721,\n",
       "  'content': '<|extra_75|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151722,\n",
       "  'content': '<|extra_76|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151723,\n",
       "  'content': '<|extra_77|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151724,\n",
       "  'content': '<|extra_78|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151725,\n",
       "  'content': '<|extra_79|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151726,\n",
       "  'content': '<|extra_80|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151727,\n",
       "  'content': '<|extra_81|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151728,\n",
       "  'content': '<|extra_82|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151729,\n",
       "  'content': '<|extra_83|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151730,\n",
       "  'content': '<|extra_84|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151731,\n",
       "  'content': '<|extra_85|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151732,\n",
       "  'content': '<|extra_86|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151733,\n",
       "  'content': '<|extra_87|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151734,\n",
       "  'content': '<|extra_88|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151735,\n",
       "  'content': '<|extra_89|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151736,\n",
       "  'content': '<|extra_90|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151737,\n",
       "  'content': '<|extra_91|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151738,\n",
       "  'content': '<|extra_92|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151739,\n",
       "  'content': '<|extra_93|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151740,\n",
       "  'content': '<|extra_94|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151741,\n",
       "  'content': '<|extra_95|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151742,\n",
       "  'content': '<|extra_96|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151743,\n",
       "  'content': '<|extra_97|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151744,\n",
       "  'content': '<|extra_98|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151745,\n",
       "  'content': '<|extra_99|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151746,\n",
       "  'content': '<|extra_100|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151747,\n",
       "  'content': '<|extra_101|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151748,\n",
       "  'content': '<|extra_102|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151749,\n",
       "  'content': '<|extra_103|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151750,\n",
       "  'content': '<|extra_104|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151751,\n",
       "  'content': '<|extra_105|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151752,\n",
       "  'content': '<|extra_106|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151753,\n",
       "  'content': '<|extra_107|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151754,\n",
       "  'content': '<|extra_108|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151755,\n",
       "  'content': '<|extra_109|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151756,\n",
       "  'content': '<|extra_110|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151757,\n",
       "  'content': '<|extra_111|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151758,\n",
       "  'content': '<|extra_112|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151759,\n",
       "  'content': '<|extra_113|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151760,\n",
       "  'content': '<|extra_114|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151761,\n",
       "  'content': '<|extra_115|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151762,\n",
       "  'content': '<|extra_116|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151763,\n",
       "  'content': '<|extra_117|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151764,\n",
       "  'content': '<|extra_118|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151765,\n",
       "  'content': '<|extra_119|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151766,\n",
       "  'content': '<|extra_120|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151767,\n",
       "  'content': '<|extra_121|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151768,\n",
       "  'content': '<|extra_122|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151769,\n",
       "  'content': '<|extra_123|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151770,\n",
       "  'content': '<|extra_124|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151771,\n",
       "  'content': '<|extra_125|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151772,\n",
       "  'content': '<|extra_126|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151773,\n",
       "  'content': '<|extra_127|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151774,\n",
       "  'content': '<|extra_128|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151775,\n",
       "  'content': '<|extra_129|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151776,\n",
       "  'content': '<|extra_130|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151777,\n",
       "  'content': '<|extra_131|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151778,\n",
       "  'content': '<|extra_132|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151779,\n",
       "  'content': '<|extra_133|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151780,\n",
       "  'content': '<|extra_134|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151781,\n",
       "  'content': '<|extra_135|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151782,\n",
       "  'content': '<|extra_136|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151783,\n",
       "  'content': '<|extra_137|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151784,\n",
       "  'content': '<|extra_138|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151785,\n",
       "  'content': '<|extra_139|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151786,\n",
       "  'content': '<|extra_140|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151787,\n",
       "  'content': '<|extra_141|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151788,\n",
       "  'content': '<|extra_142|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151789,\n",
       "  'content': '<|extra_143|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151790,\n",
       "  'content': '<|extra_144|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151791,\n",
       "  'content': '<|extra_145|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151792,\n",
       "  'content': '<|extra_146|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151793,\n",
       "  'content': '<|extra_147|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151794,\n",
       "  'content': '<|extra_148|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151795,\n",
       "  'content': '<|extra_149|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151796,\n",
       "  'content': '<|extra_150|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151797,\n",
       "  'content': '<|extra_151|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151798,\n",
       "  'content': '<|extra_152|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151799,\n",
       "  'content': '<|extra_153|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151800,\n",
       "  'content': '<|extra_154|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151801,\n",
       "  'content': '<|extra_155|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151802,\n",
       "  'content': '<|extra_156|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151803,\n",
       "  'content': '<|extra_157|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151804,\n",
       "  'content': '<|extra_158|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151805,\n",
       "  'content': '<|extra_159|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151806,\n",
       "  'content': '<|extra_160|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151807,\n",
       "  'content': '<|extra_161|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151808,\n",
       "  'content': '<|extra_162|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151809,\n",
       "  'content': '<|extra_163|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151810,\n",
       "  'content': '<|extra_164|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151811,\n",
       "  'content': '<|extra_165|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151812,\n",
       "  'content': '<|extra_166|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151813,\n",
       "  'content': '<|extra_167|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151814,\n",
       "  'content': '<|extra_168|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151815,\n",
       "  'content': '<|extra_169|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151816,\n",
       "  'content': '<|extra_170|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151817,\n",
       "  'content': '<|extra_171|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151818,\n",
       "  'content': '<|extra_172|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151819,\n",
       "  'content': '<|extra_173|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151820,\n",
       "  'content': '<|extra_174|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151821,\n",
       "  'content': '<|extra_175|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151822,\n",
       "  'content': '<|extra_176|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151823,\n",
       "  'content': '<|extra_177|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151824,\n",
       "  'content': '<|extra_178|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151825,\n",
       "  'content': '<|extra_179|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151826,\n",
       "  'content': '<|extra_180|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151827,\n",
       "  'content': '<|extra_181|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151828,\n",
       "  'content': '<|extra_182|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151829,\n",
       "  'content': '<|extra_183|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151830,\n",
       "  'content': '<|extra_184|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151831,\n",
       "  'content': '<|extra_185|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151832,\n",
       "  'content': '<|extra_186|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151833,\n",
       "  'content': '<|extra_187|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151834,\n",
       "  'content': '<|extra_188|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151835,\n",
       "  'content': '<|extra_189|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151836,\n",
       "  'content': '<|extra_190|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151837,\n",
       "  'content': '<|extra_191|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151838,\n",
       "  'content': '<|extra_192|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151839,\n",
       "  'content': '<|extra_193|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151840,\n",
       "  'content': '<|extra_194|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151841,\n",
       "  'content': '<|extra_195|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151842,\n",
       "  'content': '<|extra_196|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151843,\n",
       "  'content': '<|extra_197|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151844,\n",
       "  'content': '<|extra_198|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151845,\n",
       "  'content': '<|extra_199|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151846,\n",
       "  'content': '<|extra_200|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151847,\n",
       "  'content': '<|extra_201|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151848,\n",
       "  'content': '<|extra_202|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151849,\n",
       "  'content': '<|extra_203|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151850,\n",
       "  'content': '<|extra_204|>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151851,\n",
       "  'content': '<ref>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151852,\n",
       "  'content': '</ref>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151853,\n",
       "  'content': '<box>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151854,\n",
       "  'content': '</box>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151855,\n",
       "  'content': '<quad>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151856,\n",
       "  'content': '</quad>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151857,\n",
       "  'content': '<img>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151858,\n",
       "  'content': '</img>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True},\n",
       " {'id': 151859,\n",
       "  'content': '<imgpad>',\n",
       "  'single_word': False,\n",
       "  'lstrip': False,\n",
       "  'rstrip': False,\n",
       "  'normalized': False,\n",
       "  'special': True}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_tokens = [\n",
    "    {\n",
    "      \"id\": id,\n",
    "      \"content\": content,\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True,\n",
    "    }\n",
    "    for content, id in enc._special_tokens.items()\n",
    "  ]\n",
    "added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Type\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ImageEncoderViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 1024,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        out_chans: int = 256,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_abs_pos: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        global_attn_indexes: Tuple[int, ...] = (),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Input image size.\n",
    "            patch_size (int): Patch size.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "            depth (int): Depth of ViT.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks.\n",
    "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embed: Optional[nn.Parameter] = None\n",
    "        if use_abs_pos:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            block = Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                use_rel_pos=use_rel_pos,\n",
    "                rel_pos_zero_init=rel_pos_zero_init,\n",
    "                window_size=window_size if i not in global_attn_indexes else 0,\n",
    "                input_size=(img_size // patch_size, img_size // patch_size),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dim,\n",
    "                out_chans,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "            nn.Conv2d(\n",
    "                out_chans,\n",
    "                out_chans,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.net_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.net_3 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.neck(x.permute(0, 3, 1, 2))\n",
    "        x = self.net_2(x)\n",
    "        x = self.net_3(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
    "                use global attention.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            rel_pos_zero_init=rel_pos_zero_init,\n",
    "            input_size=input_size if window_size == 0 else (window_size, window_size),\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.attn(x)\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n",
    "\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n",
    "        (Hp, Wp): padded height and width before partition\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Hp, Wp)\n",
    "\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Window unpartition into original sequences and removing padding.\n",
    "    Args:\n",
    "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n",
    "        window_size (int): window size.\n",
    "        pad_hw (Tuple): padded height and width (Hp, Wp).\n",
    "        hw (Tuple): original height and width (H, W) before padding.\n",
    "\n",
    "    Returns:\n",
    "        x: unpartitioned sequences with [B, H, W, C].\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "\n",
    "    if Hp > H or Wp > W:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]\n",
    "\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    attn: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        attn (Tensor): attention map.\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    attn = (\n",
    "        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return attn\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: Tuple[int, int] = (16, 16),\n",
    "        stride: Tuple[int, int] = (16, 16),\n",
    "        padding: Tuple[int, int] = (0, 0),\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel_size (Tuple): kernel size of the projection layer.\n",
    "            stride (Tuple): stride of the projection layer.\n",
    "            padding (Tuple): padding size of the projection layer.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        # B C H W -> B H W C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def build_GOT_vit_b(checkpoint=None):\n",
    "    return _build_GOT_vision(\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        encoder_num_heads=12,\n",
    "        encoder_global_attn_indexes=[2, 5, 8, 11],\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_GOT_vision(\n",
    "    encoder_embed_dim,\n",
    "    encoder_depth,\n",
    "    encoder_num_heads,\n",
    "    encoder_global_attn_indexes,\n",
    "    checkpoint=None,\n",
    "):\n",
    "    prompt_embed_dim = 256\n",
    "    image_size = 1024\n",
    "    vit_patch_size = 16\n",
    "    image_embedding_size = image_size // vit_patch_size\n",
    "    image_encoder=ImageEncoderViT(\n",
    "            depth=encoder_depth,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            img_size=image_size,\n",
    "            mlp_ratio=4,\n",
    "            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "            num_heads=encoder_num_heads,\n",
    "            patch_size=vit_patch_size,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            global_attn_indexes=encoder_global_attn_indexes,\n",
    "            window_size=14,\n",
    "            out_chans=prompt_embed_dim,\n",
    "        )\n",
    "    \n",
    "\n",
    "    return image_encoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "got",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
